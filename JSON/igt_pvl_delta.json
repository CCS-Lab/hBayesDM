{
  "task_name": {
    "code": "igt",
    "desc": "Iowa Gambling Task",
    "cite": [
      "Ahn, W. Y., Busemeyer, J. R., & Wagenmakers, E. J. (2008). Comparison of decision learning models using the generalization criterion method. Cognitive Science, 32(8), 1376-1402. http://doi.org/10.1080/03640210802352992"
    ]
  },
  "model_name": {
    "code": "pvl_delta",
    "desc": "Prospect Valence Learning (PVL) Delta",
    "cite": [
      "Ahn, W. Y., Busemeyer, J. R., & Wagenmakers, E. J. (2008). Comparison of decision learning models using the generalization criterion method. Cognitive Science, 32(8), 1376-1402. http://doi.org/10.1080/03640210802352992"
    ]
  },
  "model_type": {
    "code": "",
    "desc": "Hierarchical"
  },
  "notes": [],
  "contributors": [],
  "data_columns": {
    "subjID": "A unique identifier for each subject in the data-set.",
    "choice": "Integer indicating which deck was chosen on that trial (where A==1, B==2, C==3, and D==4).",
    "gain": "Floating point value representing the amount of currency won on that trial (e.g. 50, 100).",
    "loss": "Floating point value representing the amount of currency lost on that trial (e.g. 0, -50)."
  },
  "parameters": {
    "A": {
      "desc": "learning rate",
      "info": [0, 0.5, 1]
    },
    "alpha": {
      "desc": "outcome sensitivity",
      "info": [0, 0.5, 2]
    },
    "cons": {
      "desc": "response consistency",
      "info": [0, 1, 5]
    },
    "lambda": {
      "desc": "loss aversion",
      "info": [0, 1, 10]
    }
  },
  "regressors": {},
  "postpreds": ["y_pred"],
  "additional_args": [
    {
      "code": "payscale",
      "default": 100,
      "desc": "Raw payoffs within data are divided by this number. Used for scaling data. Defaults to 100."
    }
  ]
}
