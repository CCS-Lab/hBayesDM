task_name:
  code: igt
  desc: Iowa Gambling Task
  cite:
    - Ahn, W. Y., Busemeyer, J. R., & Wagenmakers, E. J. (2008). Comparison of decision
      learning models using the generalization criterion method. Cognitive Science,
      32(8), 1376-1402. https://doi.org/10.1080/03640210802352992
model_name:
  code: orl
  desc: Outcome-Representation Learning Model
  cite:
    - "Haines, N., Vassileva, J., & Ahn, W.-Y. (2018). The Outcome-Representation Learning
      Model: A Novel Reinforcement Learning Model of the Iowa Gambling Task. Cognitive
      Science. https://doi.org/10.1111/cogs.12688"
model_type:
  code:
  desc: Hierarchical
notes:
contributors:
  - name: Nate Haines
    email: haines.175@osu.edu
    link: https://ccs-lab.github.io/team/nate-haines/
data_columns:
  subjID: A unique identifier for each subject in the data-set.
  choice:
    Integer indicating which deck was chosen on that trial (where A==1, B==2,
    C==3, and D==4).
  gain:
    Floating point value representing the amount of currency won on that trial
    (e.g. 50, 100).
  loss:
    Floating point value representing the amount of currency lost on that trial
    (e.g. 0, -50).
parameters:
  Arew:
    desc: reward learning rate
    info: [0, 0.1, 1]
  Apun:
    desc: punishment learning rate
    info: [0, 0.1, 1]
  K:
    desc: perseverance decay
    info: [0, 0.1, 5]
  betaF:
    desc: outcome frequency weight
    info: [-Inf, 0.1, Inf]
  betaP:
    desc: perseverance weight
    info: [-Inf, 1, Inf]
regressors:
postpreds:
  - y_pred
additional_args:
  - code: payscale
    default: 100
    desc:
      Raw payoffs within data are divided by this number. Used for scaling data.
      Defaults to 100.
