task_name:
  code: igt
  desc: Iowa Gambling Task
  cite:
    - Ahn, W. Y., Busemeyer, J. R., & Wagenmakers, E. J. (2008). Comparison of decision
      learning models using the generalization criterion method. Cognitive Science,
      32(8), 1376-1402. https://doi.org/10.1080/03640210802352992
model_name:
  code: vpp
  desc: Value-Plus-Perseverance
  cite:
    - "Worthy, D. A., & Todd Maddox, W. (2013). A comparison model of reinforcement-learning
      and win-stay-lose-shift decision-making processes: A tribute to W.K. Estes. Journal
      of Mathematical Psychology, 59, 41-49. https://doi.org/10.1016/j.jmp.2013.10.001"
model_type:
  code:
  desc: Hierarchical
notes:
contributors:
data_columns:
  subjID: A unique identifier for each subject in the data-set.
  choice:
    Integer indicating which deck was chosen on that trial (where A==1, B==2,
    C==3, and D==4).
  gain:
    Floating point value representing the amount of currency won on that trial
    (e.g. 50, 100).
  loss:
    Floating point value representing the amount of currency lost on that trial
    (e.g. 0, -50).
parameters:
  A:
    desc: learning rate
    info: [0, 0.5, 1]
  alpha:
    desc: outcome sensitivity
    info: [0, 0.5, 2]
  cons:
    desc: response consistency
    info: [0, 1, 5]
  lambda:
    desc: loss aversion
    info: [0, 1, 10]
  epP:
    desc: gain impact
    info: [-Inf, 0, Inf]
  epN:
    desc: loss impact
    info: [-Inf, 0, Inf]
  K:
    desc: decay rate
    info: [0, 0.5, 1]
  w:
    desc: RL weight
    info: [0, 0.5, 1]
regressors:
postpreds:
  - y_pred
additional_args:
  - code: payscale
    default: 100
    desc:
      Raw payoffs within data are divided by this number. Used for scaling data.
      Defaults to 100.
