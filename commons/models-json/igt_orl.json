{
  "task_name": {
    "code": "igt",
    "desc": "Iowa Gambling Task",
    "cite": [
      "Ahn, W. Y., Busemeyer, J. R., & Wagenmakers, E. J. (2008). Comparison of decision learning models using the generalization criterion method. Cognitive Science, 32(8), 1376-1402. http://doi.org/10.1080/03640210802352992"
    ]
  },
  "model_name": {
    "code": "orl",
    "desc": "Outcome-Representation Learning Model",
    "cite": [
      "Haines, N., Vassileva, J., & Ahn, W.-Y. (2018). The Outcome-Representation Learning Model: A Novel Reinforcement Learning Model of the Iowa Gambling Task. Cognitive Science. https://doi.org/10.1111/cogs.12688"
    ]
  },
  "model_type": {
    "code": "",
    "desc": "Hierarchical"
  },
  "notes": [],
  "contributors": [
    {
      "name": "Nate Haines",
      "email": "haines.175@osu.edu",
      "link": "https://ccs-lab.github.io/team/nate-haines/"
    }
  ],
  "data_columns": {
    "subjID": "A unique identifier for each subject in the data-set.",
    "choice": "Integer indicating which deck was chosen on that trial (where A==1, B==2, C==3, and D==4).",
    "gain": "Floating point value representing the amount of currency won on that trial (e.g. 50, 100).",
    "loss": "Floating point value representing the amount of currency lost on that trial (e.g. 0, -50)."
  },
  "parameters": {
    "Arew": {
      "desc": "reward learning rate",
      "info": [0, 0.1, 1]
    },
    "Apun": {
      "desc": "punishment learning rate",
      "info": [0, 0.1, 1]
    },
    "K": {
      "desc": "perseverance decay",
      "info": [0, 0.1, 5]
    },
    "betaF": {
      "desc": "outcome frequency weight",
      "info": ["-Inf", 0.1, "Inf"]
    },
    "betaP": {
      "desc": "perseverance weight",
      "info": ["-Inf", 1, "Inf"]
    }
  },
  "regressors": {},
  "postpreds": ["y_pred"],
  "additional_args": [
    {
      "code": "payscale",
      "default": 100,
      "desc": "Raw payoffs within data are divided by this number. Used for scaling data. Defaults to 100."
    }
  ]
}
