<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Hierarchical Bayesian Analysis on Hierarchical Gaussian Filter • hBayesDM</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js" integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/flatly/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet">
<meta property="og:title" content="Hierarchical Bayesian Analysis on Hierarchical Gaussian Filter">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">


    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">hBayesDM</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">1.3.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../articles/index.html">Articles</a>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/CCS-Lab/hBayesDM/" class="external-link">
    <span class="fab fa-github fa-lg"></span>

  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->



      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Hierarchical Bayesian Analysis on Hierarchical
Gaussian Filter</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/CCS-Lab/hBayesDM/blob/test/vignettes/hgf_tutorial.Rmd" class="external-link"><code>vignettes/hgf_tutorial.Rmd</code></a></small>
      <div class="hidden name"><code>hgf_tutorial.Rmd</code></div>

    </div>

    
    
<p>By <a href="https://github.com/bugoverdose" class="external-link"><strong>Jinwoo
Jeong</strong></a>, <a href="https://github.com/juhajulia" class="external-link"><strong>Juha
Lee</strong></a>, <a href="https://github.com/0150362" class="external-link"><strong>Yusom
Jo</strong></a>, <a href="https://ccs-lab.github.io/team/young-ahn/" class="external-link"><strong>Woo-Young
Ahn</strong></a> | September 2, 2025</p>
<p><br></p>
<p><strong>Hierarchical Gaussian Filter (HGF)</strong> is a
computational model designed to explain how individuals learn in
uncertain and changing environments <span class="citation">(Mathys et
al., 2011)</span>. Currently, <a href="https://github.com/ComputationalPsychiatry" class="external-link">TAPAS</a> is the most
widely used tool for applying HGF to behavioral data. By offering
various combinations of perceptual and observation models, <a href="https://github.com/ComputationalPsychiatry/hgf-toolbox" class="external-link">TAPAS HGF
Toolbox</a> enabled a computationally efficient way to apply different
HGF models (also see <a href="https://github.com/ComputationalPsychiatry/pyhgf" class="external-link">PyHGF</a>, a
Python-library, which supports generalized HGF models).</p>
<p>Here, we show how we implemented HGF in the hBayesDM
<code>1.3.0</code> <span class="citation">(Ahn et al., 2017)</span>. Two
new HGF models are included in the package: <a href="../reference/hgf_ibrb.html"><code>hgf_ibrb</code></a> (for
hierarchical Bayesian analysis) and <a href="../reference/hgf_ibrb_single.html"><code>hgf_ibrb_single</code></a>
(for individual Bayesian analysis). <code>ibrb</code> refers to input =
binary &amp; response = binary. Although these functions are currently
limited to behavioral data with binary inputs and binary responses, they
provide a straightforward way to apply MCMC and (hierarchical) Bayesian
analysis to HGF models, which are currently unavailable in TAPAS.</p>
<div class="section level2">
<h2 id="example-task">1. Example Task<a class="anchor" aria-label="anchor" href="#example-task"></a>
</h2>
<p>Let’s consider a probabilistic reversal learning task where HGF can
be applied.</p>
<p>On each trial, participants are presented with two colored
options—blue(1) and orange(0)—and choose one of the options. Reward
contingency changes over trials (see Figure 1) and participants are
instructed to maximize the cumulative reward.</p>
<div class="figure" style="text-align: center">
<img src="images/hgf_tutorial/contingency_schedule.png" alt="Figure 1. Reward contingency schedule. The y-axis shows the actual reward probability of the blue option p(reward|blue), while the x-axis shows the trial number. Note that p(reward|orange) = 1 - p(reward|blue)." width="70%"><p class="caption">
Figure 1. Reward contingency schedule. The y-axis shows the actual
reward probability of the blue option p(reward|blue), while the x-axis
shows the trial number. Note that p(reward|orange) = 1 - p(reward|blue).
</p>
</div>
<p>The figure above illustrates a contingency schedule for a restless
two-armed bandit task. The schedule spans 400 trials, alternating
between stable and volatile phases.</p>
<ul>
<li>Stable phases (100 trials each): reward probabilities remain fixed.
For example, choosing the blue option yields a reward with a probability
of 80% throughout the first stable block.</li>
<li>Volatile phases (100 trials each): reward probabilities switch
between 20% and 80% after a random number of trials (6, 10, 14, or 20),
making the environment unpredictable.</li>
</ul>
<p>Probabilistic learning tasks like this are well suited for HGF
analysis because the environment changes over time, requiring
participants to continuously track shifting reward contingencies.</p>
</div>
<div class="section level2">
<h2 id="understanding-hgf">2. Understanding HGF<a class="anchor" aria-label="anchor" href="#understanding-hgf"></a>
</h2>
<p>In this section, we briefly explain HGF (please see Mathys et al.,
2011, 2014 for more details). One way to approach probabilistic learning
tasks is to assume a “true” hidden quantity for each trial, such as the
actual probability of receiving a reward from the chosen option. This
hidden state changes over time according to the contingency schedule,
but the agent cannot observe it directly. Instead, the agent tries to
update its beliefs about the hidden state across trials based on inputs
and observed feedback (whether the chosen option led to a reward or
not). In this sense, the task represents a learning problem under
uncertainty: how an agent can keep track of a changing, probabilistic
world based on noisy feedback.</p>
<p>Note that there exist multiple sources of uncertainty:
<strong>stochasticity</strong> (the reward is probabilistic on each
trial), <strong>volatility</strong> (the reward contingency changes over
time), and even <strong>volatility of volatility</strong> (the degree of
environmental change itself may vary over time). Successful performance
on such a task therefore depends on the optimal processing of these
sources of uncertainty in learning, and the pattern of such processing
can be characterized by the parameter values in HGF.</p>
<div class="section level3">
<h3 id="model-structure">2.1. Model Structure<a class="anchor" aria-label="anchor" href="#model-structure"></a>
</h3>
<p>As the name suggests, the Hierarchical Gaussian Filter consists of
multiple levels of hidden states, each of which are organized in a
hierarchy and evolving in Gaussian random walks. Theoretically, the
number of hierarchy levels has no upper bound, but in practice it is not
set too high due to diminishing explanatory gains and limited
interpretability as a cognitive model.</p>
<div class="figure" style="text-align: center">
<img src="images/hgf_tutorial/hgf_gaussian_random_walk.png" alt="Figure 2. Overview of the Hierarchical Gaussian Filter (adopted from Mathys et al. 2014)" width="70%"><p class="caption">
Figure 2. Overview of the Hierarchical Gaussian Filter (adopted from
Mathys et al. 2014)
</p>
</div>
<p>For instance, let’s assume a 3-level HGF with binary inputs and
binary responses. An HGF model with 3 levels has three (hidden) states
for each level: x₁, x₂, x₃. And each state represents a different aspect
of the environment.</p>
<ul>
<li><p><strong>Level 1 (x₁)</strong> is the lowest level, representing
the immediate observation or perceptual state. For example, in a binary
task, x₁ might be the state of a cue or outcome coded as 0 or 1. This
level does not evolve via a random walk, instead, it is generated from
x₂ through an observation model (e.g., Bernoulli for binary outcomes).
When there is no sensory noise, x₁ directly corresponds to the observed
input.</p></li>
<li><p><strong>Level 2 (x₂)</strong> is a hidden state that represents
the <strong>current contingency of the environment</strong> (e.g., the
probability of a binary outcome). It can be thought of as the agent’s
belief about the latent factor governing level-1 events. This is a
continuous state that drifts over time via a Gaussian random walk: on
each trial, x₂ changes slightly, with the variance of this change
dictated by level 3 (x₃).</p></li>
<li><p><strong>Level 3 (x₃)</strong> is a higher-level hidden state
representing the <strong>volatility of the environment</strong>, namely
how rapidly x₂ changes. Although x₃ is modeled as a Gaussian random
walk, the variance of this random walk is determined by a constant
parameter because there is no higher level to modulate it. Intuitively,
when x₃ is high, the agent believes the environment is changing rapidly
and therefore updates x₂ more strongly; when x₃ is low, the agent
assumes stability and updates x₂ more conservatively.</p></li>
</ul>
<div class="figure" style="text-align: center">
<img src="images/hgf_tutorial/hgf_levels.png" alt="Figure 3. Overview of the hierarchical generative model (Mathys et al. 2011)" width="70%"><p class="caption">
Figure 3. Overview of the hierarchical generative model (Mathys et
al. 2011)
</p>
</div>
</div>
<div class="section level3">
<h3 id="perceptual-model">2.2. Perceptual model<a class="anchor" aria-label="anchor" href="#perceptual-model"></a>
</h3>
<p>In the HGF perceptual model, the parameters κ (kappa) and ω (omega)
regulate the coupling and dynamics across hierarchical levels. They
encode the agent’s prior assumptions about environmental uncertainty and
determine how learning adapts to changing conditions.</p>
<p>At each level <span class="math inline">\(l\)</span>, the state <span class="math inline">\(x_l\)</span> at trial <span class="math inline">\(k\)</span> evolves as a Gaussian random walk whose
variance depends on κ and ω:</p>
<p><span class="math display">\[\begin{align*}
x_l^{(k)} &amp;\sim N\bigl(x_l^{(k-1)} \text{, } \exp(\kappa_l
x_{l+1}^{(k)} + \omega_l )\bigr), \quad l=2,...L-1 \\
\end{align*}\]</span></p>
<p>At the top level <span class="math inline">\(L\)</span>, the variance
depends only on ω:</p>
<p><span class="math display">\[\begin{align*}
x_L^{(k)} &amp;\sim N\bigl(x_L^{(k-1)} \text{, } \exp(\omega_L )\bigr)
\end{align*}\]</span></p>
<div class="section level4">
<h4 id="κ-phasic-volatility">κ: phasic volatility<a class="anchor" aria-label="anchor" href="#%CE%BA-phasic-volatility"></a>
</h4>
<p>κ determines how strongly a lower-level state is influenced by the
state above it. Formally, it scales the effect of the higher-level value
on the variance of the lower-level random walk. In a perception model
with binary inputs and <span class="math inline">\(L\)</span>
hierarchical levels, κ is defined for levels 2 through <span class="math inline">\(L-1\)</span>.</p>
<p>κ must be non-negative, and it has been suggested that an upper bound
at or below 2 is sensible <span class="citation">(Mathys et al.,
2014)</span>.</p>
<ul>
<li>When κ is too small, the lower level is almost decoupled from the
higher one. Even if the higher-level state signals increased volatility,
the learning rate at the lower level hardly changes. The agent ends up
learning at a nearly fixed pace regardless of environmental
instability.</li>
<li>When κ is too large, even small changes in the higher-level state
strongly alter the lower-level learning rate. For instance, if
volatility spikes, the agent immediately increases its learning rate.
This makes the agent very adaptive, but it can also lead to overreacting
to random noise.</li>
</ul>
</div>
<div class="section level4">
<h4 id="ω-tonic-volatility">ω: tonic volatility<a class="anchor" aria-label="anchor" href="#%CF%89-tonic-volatility"></a>
</h4>
<p>ω provides the baseline drift for each level’s random walk. It is the
constant offset in the log-variance, setting how much the agent expects
change even without higher-level modulation. In a perception model with
binary inputs and <span class="math inline">\(L\)</span> hierarchical
levels, ω is defined for levels 2 through <span class="math inline">\(L\)</span>.</p>
<p>Theoretically, ω doesn’t require upper or lower bounds. However, both
<a href="../reference/hgf_ibrb.html"><code>hgf_ibrb</code></a> and <a href="../reference/hgf_ibrb_single.html"><code>hgf_ibrb_single</code></a>
require setting the appropriate range for each level of ω parameters for
efficient and effective sampling.</p>
<ul>
<li>When ω is too low, the random walk variance is small by default. The
agent assumes stability, so updates are slow and conservative. Even with
repeated prediction errors, beliefs change only gradually, reflecting a
tendency to cling to prior expectations.</li>
<li>When ω is too high, the random walk variance is very large by
default. The agent assumes that environmental changes are common, so it
updates beliefs quickly, even in a stable environment. This can capture
individuals’ characteristics who are highly responsive or prone to
“jumping to conclusions.”</li>
</ul>
</div>
</div>
<div class="section level3">
<h3 id="response-model">2.3. Response model<a class="anchor" aria-label="anchor" href="#response-model"></a>
</h3>
<p>The response model maps the agent’s internal prediction onto an
observable decision. In both <a href="../reference/hgf_ibrb.html"><code>hgf_ibrb</code></a> and <a href="../reference/hgf_ibrb_single.html"><code>hgf_ibrb_single</code></a>,
the unit-square sigmoid function is used to transform a predictive
probability 𝑚∈[0,1] into a binary choice probability:</p>
<p><span class="math display">\[
p(y = 1) = \frac{m^{\text{ }\zeta}}{m^{\text{ }\zeta} + (1 - m)^{\zeta}}
\]</span></p>
<div class="section level4">
<h4 id="ζ-inverse-decision-noise">ζ: inverse decision noise<a class="anchor" aria-label="anchor" href="#%CE%B6-inverse-decision-noise"></a>
</h4>
<p>The parameter ζ (zeta) controls the steepness of the unit-square
sigmoid, regulating the stochasticity of decision-making. ζ is defined
only for level 1 and must be non-negative.</p>
<ul>
<li>When ζ is low, the sigmoid curve becomes shallow. Even strong
predictions (e.g., 𝑚 = 0.8) yield only moderate choice biases, and
choices remain noisy and weakly belief-driven.</li>
<li>When ζ is high, the sigmoid curve becomes steep. Even very small
deviations from 0.5 lead to near-deterministic choices, and behavior
closely follows the agent’s beliefs.</li>
</ul>
</div>
</div>
</div>
<div class="section level2">
<h2 id="tutorial-on-hgf_ibrb-and-hgf_ibrb_single">3. Tutorial on <code>hgf_ibrb</code> and
<code>hgf_ibrb_single</code><a class="anchor" aria-label="anchor" href="#tutorial-on-hgf_ibrb-and-hgf_ibrb_single"></a>
</h2>
<p>This section explains how we can actually run <a href="../reference/hgf_ibrb.html"><code>hgf_ibrb</code></a> and <a href="../reference/hgf_ibrb_single.html"><code>hgf_ibrb_single</code></a>.
The examples are based on the R version of the <code>hBayesDM</code>
package, but both functions are also implemented in the Python version
of the <code>hBayesDM</code> package.</p>
<div class="section level3">
<h3 id="setup">3.1. Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h3>
<div class="section level4">
<h4 id="hbayesdm-setup">hBayesDM setup<a class="anchor" aria-label="anchor" href="#hbayesdm-setup"></a>
</h4>
<p>Check the <a href=".//getting_started.html">Getting Started</a> page
to set up <code>hBayesDM</code>.</p>
</div>
<div class="section level4">
<h4 id="prepare-your-data">Prepare your data<a class="anchor" aria-label="anchor" href="#prepare-your-data"></a>
</h4>
<p>For simple tests, <code>hBayesDM</code> has example data for both <a href="../reference/hgf_ibrb.html"><code>hgf_ibrb</code></a> and <a href="../reference/hgf_ibrb_single.html"><code>hgf_ibrb_single</code></a>.</p>
<p>If you want to apply HGF on your own data, make sure that your file
has the <code>.csv</code> or <code>.tsv</code> format and contains these
four columns.</p>
<ul>
<li>
<code>subjID</code> : unique subject identifier (integer or
string)</li>
<li>
<code>trialNum</code> : trial index (1, 2, 3, …)</li>
<li>
<code>u</code> : input on that trial (0 or 1)</li>
<li>
<code>y</code> : subject’s choice on that trial (0 or 1)</li>
</ul>
<p>For example, the task design in <a href="#example-task">Section 1</a>
provides two colored options: blue coded as 1 and orange coded as 0. In
this case, a participant (<code>subjID = 1</code>) can choose the blue
option (<code>y = 1</code>) at the first trial
(<code>trialNum = 1</code>) when the correct choice was actually the
orange option (<code>u = 1</code>). No other information (e.g.,
<code>feedback</code> column indicating whether the subject got the
reward or not) is needed in the HGF model.</p>
</div>
</div>
<div class="section level3">
<h3 id="fitting-with-default-settings">3.2. Fitting with default settings<a class="anchor" aria-label="anchor" href="#fitting-with-default-settings"></a>
</h3>
<p>Now, we will share various ways we can fit <a href="../reference/hgf_ibrb.html"><code>hgf_ibrb</code></a> and <a href="../reference/hgf_ibrb_single.html"><code>hgf_ibrb_single</code></a>
on the example data.</p>
<p>Below is the simplest way to run <a href="../reference/hgf_ibrb.html"><code>hgf_ibrb</code></a> on a dataset
with multiple participants. The command below initiates an MCMC
procedure of 4 MCMC chains, each consisting of 5 burn-in (warm-up)
iterations followed by 5 sampling iterations.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hgf_ibrb.html">hgf_ibrb</a></span><span class="op">(</span>data <span class="op">=</span> <span class="st">"example"</span>, niter <span class="op">=</span> <span class="fl">1000</span>, nwarmup <span class="op">=</span> <span class="fl">500</span>, nchain <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span></code></pre></div>
<p>The above command is the same as the commands below because
unspecified arguments fall back to their default values. By default, <a href="../reference/hgf_ibrb.html"><code>hgf_ibrb</code></a> fits a
3-level HGF model (<code>L = 3</code>). The <code>kappa_lower</code> and
<code>kappa_upper</code> arguments are one-element vectors that bound
κ₂, whereas <code>omega_lower</code> and <code>omega_upper</code> are
two-element vectors that bound ω₂ and ω₃. In contrast,
<code>zeta_lower</code> and <code>zeta_upper</code> are scalars, since
there is only a single ζ regardless of the hierarchy depth.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hgf_ibrb.html">hgf_ibrb</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="st">"example"</span>,</span>
<span>  niter <span class="op">=</span> <span class="fl">1000</span>,</span>
<span>  nwarmup <span class="op">=</span> <span class="fl">500</span>,</span>
<span>  nchain <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  L <span class="op">=</span> <span class="fl">3</span>,</span>
<span>  input_first <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  mu0 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.5</span>, <span class="fl">1.0</span><span class="op">)</span>,</span>
<span>  sigma0 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">1.0</span><span class="op">)</span>,</span>
<span>  kappa_lower <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span>,  </span>
<span>  kappa_upper <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span><span class="op">)</span>,</span>
<span>  omega_lower <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">10</span>, <span class="op">-</span><span class="fl">15</span><span class="op">)</span>, </span>
<span>  omega_upper <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>,</span>
<span>  zeta_lower  <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  zeta_upper  <span class="op">=</span> <span class="fl">2</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Below are some of the simple ways to print the overall statistics of
the fitted results.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">fit</span><span class="op">$</span><span class="va">allIndPars</span><span class="op">)</span></span></code></pre></div>
<!-- $ -->
<p>To apply HGF to a single participant’s data, you can use <a href="../reference/hgf_ibrb_single.html"><code>hgf_ibrb_single</code></a>
like below.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hgf_ibrb_single.html">hgf_ibrb_single</a></span><span class="op">(</span>data <span class="op">=</span> <span class="st">"example"</span>, niter <span class="op">=</span> <span class="fl">1000</span>, nwarmup <span class="op">=</span> <span class="fl">500</span>, nchain <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">fit</span><span class="op">$</span><span class="va">allIndPars</span><span class="op">)</span></span></code></pre></div>
<!-- $ -->
</div>
<div class="section level3">
<h3 id="input_first-option">3.3. <code>input_first</code> option<a class="anchor" aria-label="anchor" href="#input_first-option"></a>
</h3>
<p>The <code>input_first</code> option is to help researchers apply HGF
without changing their data format.</p>
<ul>
<li>input_first=TRUE: for each row, participant observed the value of
input <code>u</code> before choosing <code>y</code>
</li>
<li>input_first=FALSE: for each row, participant observed the value of
input <code>u</code> after choosing <code>y</code> (default)</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hgf_ibrb.html">hgf_ibrb</a></span><span class="op">(</span>data <span class="op">=</span> <span class="st">"example"</span>, niter <span class="op">=</span> <span class="fl">1000</span>, nwarmup <span class="op">=</span> <span class="fl">500</span>, nchain <span class="op">=</span> <span class="fl">4</span>, input_first<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p>Similar features are also implemented in the observation models in
TAPAS as the <code>predorpost</code> configuration.</p>
</div>
<div class="section level3">
<h3 id="parameter-boundaries">3.4. Parameter boundaries<a class="anchor" aria-label="anchor" href="#parameter-boundaries"></a>
</h3>
<p>Parameter boundaries can be set for each parameter. For example, if
you want to set parameter boundaries for a 3-level HGF model as:</p>
<p><span class="math display">\[\begin{align*}
0 &lt; &amp;\text{ }\kappa &lt; 1 \\
-10 &lt; &amp;\text{ }\omega_{2} &lt; 2 \\
-15 &lt; &amp;\text{ }\omega_{3} &lt; 3 \\
0 &lt; &amp;\text{ }\zeta &lt; 4
\end{align*}\]</span></p>
<p>Then, you set the arguments below:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hgf_ibrb.html">hgf_ibrb</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="st">"example"</span>,</span>
<span>  kappa_lower <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span>,</span>
<span>  kappa_upper <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span>,</span>
<span>  omega_lower <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">10</span>, <span class="op">-</span><span class="fl">15</span><span class="op">)</span>,</span>
<span>  omega_upper <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">3</span><span class="op">)</span>,</span>
<span>  zeta_lower  <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  zeta_upper  <span class="op">=</span> <span class="fl">4</span></span>
<span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="fixing-parameter-values">3.5. Fixing parameter values<a class="anchor" aria-label="anchor" href="#fixing-parameter-values"></a>
</h3>
<p>As TAPAS provides a way to fixate a certain parameter to a fixed
value, <a href="../reference/hgf_ibrb.html"><code>hgf_ibrb</code></a>
and <a href="../reference/hgf_ibrb_single.html"><code>hgf_ibrb_single</code></a>
also provide a similar feature. To set a parameter as a constant, simply
set the lower bound and upper bound to the same value. Below are some
examples.</p>
<p>The command below fixates κ value to 1, while other parameters are
set as free parameters with their default boundaries.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hgf_ibrb.html">hgf_ibrb</a></span><span class="op">(</span>data <span class="op">=</span> <span class="st">"example"</span>, kappa_lower <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1.0</span><span class="op">)</span>, kappa_upper <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1.0</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>The command below fixates ω₂ to -4, while ω₃ is still estimated with
the given boundaries (-10 &lt; ω₃ &lt; -2).</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hgf_ibrb.html">hgf_ibrb</a></span><span class="op">(</span>data <span class="op">=</span> <span class="st">"example"</span>, omega_lower <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">4.0</span>, <span class="op">-</span><span class="fl">10.0</span><span class="op">)</span>, omega_upper <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">4.0</span>, <span class="op">-</span><span class="fl">2.0</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>In contrast, the command below fixates ω₃ to -4, while ω₂ is still to
be estimated with the given boundaries (-9 &lt; ω₂ &lt; -1).</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hgf_ibrb.html">hgf_ibrb</a></span><span class="op">(</span>data <span class="op">=</span> <span class="st">"example"</span>, omega_lower <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">9.0</span>, <span class="op">-</span><span class="fl">4.0</span><span class="op">)</span>, omega_upper <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1.0</span>, <span class="op">-</span><span class="fl">4.0</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>The command below fixates ζ value to 1, while other parameters are
estimated with default boundaries.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hgf_ibrb.html">hgf_ibrb</a></span><span class="op">(</span>data <span class="op">=</span> <span class="st">"example"</span>, zeta_lower <span class="op">=</span> <span class="fl">1.0</span>, zeta_upper <span class="op">=</span> <span class="fl">1.0</span><span class="op">)</span></span></code></pre></div>
<p>Of course, you can fixate multiple parameters at the same time as
shown below:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hgf_ibrb.html">hgf_ibrb</a></span><span class="op">(</span>data <span class="op">=</span> <span class="st">"example"</span>, kappa_lower <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1.0</span><span class="op">)</span>, kappa_upper <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1.0</span><span class="op">)</span>, zeta_lower <span class="op">=</span> <span class="fl">1.0</span>, zeta_upper <span class="op">=</span> <span class="fl">1.0</span><span class="op">)</span></span>
<span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hgf_ibrb.html">hgf_ibrb</a></span><span class="op">(</span>data <span class="op">=</span> <span class="st">"example"</span>, omega_lower <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">6.0</span>, <span class="op">-</span><span class="fl">4.0</span><span class="op">)</span>, omega_upper <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">6.0</span>, <span class="op">-</span><span class="fl">4.0</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="l-level-of-hierarchy">3.6. <code>L</code>: level of hierarchy<a class="anchor" aria-label="anchor" href="#l-level-of-hierarchy"></a>
</h3>
<p><code>L</code> option can be used to specify the level of hierarchy.
The default and the minimum value is <code>L=3</code> because it is the
most widely used and most interpretable.</p>
<p>Note that the number of parameters is automatically chosen based on
the level of hierarchy. For example, the code below will fail because
the default arguments for each parameter can only be used for
<code>L = 3</code>.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hgf_ibrb.html">hgf_ibrb</a></span><span class="op">(</span>data <span class="op">=</span> <span class="st">"example"</span>, niter <span class="op">=</span> <span class="fl">1000</span>, nwarmup <span class="op">=</span> <span class="fl">500</span>, nchain <span class="op">=</span> <span class="fl">4</span>, L <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span></code></pre></div>
<p>Below is an example of fitting a 4-level HGF model. <code>L=4</code>
introduces a fourth latent state (x₄) and extends the parameterization
with two couplings (κ₂, κ₃), volatilities (ω₂, ω₃, ω₄), and
initial-state vectors for levels 2-4.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hgf_ibrb.html">hgf_ibrb</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="st">"example"</span>,</span>
<span>  niter <span class="op">=</span> <span class="fl">1000</span>,</span>
<span>  nwarmup <span class="op">=</span> <span class="fl">500</span>,</span>
<span>  nchain <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  L <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  kappa_lower <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.0</span>, <span class="fl">0.0</span><span class="op">)</span>,</span>
<span>  kappa_upper <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">3.0</span>, <span class="fl">3.0</span><span class="op">)</span>,</span>
<span>  omega_lower <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">9.0</span>, <span class="op">-</span><span class="fl">10.0</span>, <span class="op">-</span><span class="fl">12.0</span><span class="op">)</span>,</span>
<span>  omega_upper <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1.0</span>,  <span class="op">-</span><span class="fl">2.0</span>,  <span class="op">-</span><span class="fl">3.0</span><span class="op">)</span>,</span>
<span>  mu0    <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.0</span>, <span class="fl">1.0</span>, <span class="fl">1.0</span><span class="op">)</span>,</span>
<span>  sigma0 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">1.0</span>, <span class="fl">1.0</span><span class="op">)</span>,</span>
<span>  zeta_lower  <span class="op">=</span> <span class="fl">0.03</span>,</span>
<span>  zeta_upper  <span class="op">=</span> <span class="fl">5.0</span></span>
<span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="parameter-recovery">4. Parameter Recovery<a class="anchor" aria-label="anchor" href="#parameter-recovery"></a>
</h2>
<p>In order to ensure that the estimated parameters genuinely reflect
the underlying generative process, parameter recovery analysis was
conducted to validate parameter identifiability and estimation
reliability. This procedure involves generating synthetic datasets from
known parameter values and then re-estimating those parameters using the
same model <span class="citation">(Ahn et al., 2011; Heathcote et al.,
2015; Wilson &amp; Collins, 2019)</span>.</p>
<div class="section level3">
<h3 id="simulation">4.1. Simulation<a class="anchor" aria-label="anchor" href="#simulation"></a>
</h3>
<p>In order to simulate binary inputs, the contingency schedule from <a href="#example-task">Section 1</a> was used to create 400 trials of
inputs. To simulate binary responses, <code>tapas_simModel</code>
function from TAPAS HGF Toolbox was applied to the simulated inputs,
assuming 3-level HGF with binary responses. A total of 30 participants’
behavioral datasets were generated using randomly chosen parameter
combinations:</p>
<p><span class="math display">\[\begin{align*}
\kappa &amp;= 1.0 \\
-9.0 \leq &amp;\text{ }\omega_{2} \leq -1.0 \\
-10.0 \leq &amp;\text{ }\omega_{3} \leq -2.0 \\
0.03 \leq &amp;\text{ }\zeta \leq 4.0
\end{align*}\]</span></p>
<p>Note that κ is fixed to 1, following many of the previous works with
similar context <span class="citation">(Hein et al., 2021; Tecilla et
al., 2023)</span>.</p>
</div>
<div class="section level3">
<h3 id="recovery-with-tapas">4.2. Recovery with TAPAS<a class="anchor" aria-label="anchor" href="#recovery-with-tapas"></a>
</h3>
<p>In order to check whether the simulated data had reasonable parameter
ranges that HGF models can recover, <code>tapas_fitModel</code> function
from TAPAS HGF Toolbox was applied to check parameter recovery.
<code>tapas_hgf_binary</code> and <code>tapas_unitsq_sgm</code> models
were applied using the following prior settings:</p>
<p><span class="math display">\[\begin{align*}
\mu_{0,2} &amp;= 0, \quad \mu_{0,3} = 1 \\
\sigma^{2}_{0,2} &amp;= 1, \quad \sigma^{2}_{0,3} = 1 \\
\omega_{2} &amp;\sim N(-5, 4^2) \\
\omega_{3} &amp;\sim N(-6, 4^2)
\end{align*}\]</span></p>
<p>Figure 4 shows that both ω₂ and ζ were recovered extremely well (ω₂:
r = 0.99, p &lt; 1.63e-23; ζ: r = 0.99, p &lt; 8.9e-26), indicating that
these parameters can be reliably estimated at the individual level. In
contrast, ω₃ exhibited exhibited poor parameter recovery (r = -0.09, p =
0.633), with all the estimated values presumably stuck in a local minima
near -6.</p>
<div class="figure" style="text-align: center">
<img src="images/hgf_tutorial/tapas_recovery.png" alt="Figure 4. Parameter recovery with TAPAS. Posterior means of the estimated parameters." width="90%"><p class="caption">
Figure 4. Parameter recovery with TAPAS. Posterior means of the
estimated parameters.
</p>
</div>
</div>
<div class="section level3">
<h3 id="recovery-with-hgf_ibrb_single">4.3. Recovery with <code>hgf_ibrb_single</code><a class="anchor" aria-label="anchor" href="#recovery-with-hgf_ibrb_single"></a>
</h3>
<p>For each subject’s data, <a href="../reference/hgf_ibrb_single.html"><code>hgf_ibrb_single</code></a>
was applied in a loop with settings like below.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hgf_ibrb_single.html">hgf_ibrb_single</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">subject_data</span>,</span>
<span>                       niter <span class="op">=</span> <span class="fl">3000</span>,</span>
<span>                       nwarmup <span class="op">=</span> <span class="fl">1500</span>,</span>
<span>                       nchain <span class="op">=</span> <span class="fl">4</span>,</span>
<span>                       L <span class="op">=</span> <span class="fl">3</span>,</span>
<span>                       kappa_lower <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1.0</span><span class="op">)</span>,</span>
<span>                       kappa_upper <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1.0</span><span class="op">)</span>,</span>
<span>                       omega_lower <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">9.0</span>, <span class="op">-</span><span class="fl">10.0</span><span class="op">)</span>,</span>
<span>                       omega_upper <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1.0</span>, <span class="op">-</span><span class="fl">2.0</span><span class="op">)</span>,</span>
<span>                       zeta_lower  <span class="op">=</span> <span class="fl">0.03</span>,</span>
<span>                       zeta_upper  <span class="op">=</span> <span class="fl">4.0</span>,</span>
<span>                       mu0 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.0</span>, <span class="fl">1.0</span><span class="op">)</span>,</span>
<span>                       inits <span class="op">=</span> <span class="st">"random"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="images/hgf_tutorial/individual_recovery.png" alt="Figure 5. Parameter recovery with hBayesDM (hgf_ibrb_single, individual Bayesian analysis)" width="90%"><p class="caption">
Figure 5. Parameter recovery with hBayesDM (hgf_ibrb_single, individual
Bayesian analysis)
</p>
</div>
<p>Figure 5 shows that both ω₂ and ζ were recovered pretty well (ω₂: r =
0.90, p &lt; 2.51e-11; ζ: r = 0.95, p &lt; 7.81e-16). In contrast, ω₃
showed poor parameter recovery (r = 0.03, p = 0.892).</p>
<div class="figure" style="text-align: center">
<img src="images/hgf_tutorial/tapas_individual_recovery.png" alt="Figure 6. TAPAS vs hgf_ibrb_single" width="90%"><p class="caption">
Figure 6. TAPAS vs hgf_ibrb_single
</p>
</div>
<p>In Figure 6, we compared TAPAS and <code>hBayesDM</code>
(individual-level) parameter estimates. The figure shows that ω₂ and ζ
parameter estimates of <code>hBayesDM</code> and TAPAS are highly
correlated with each other. In contrast, ω₃ estimates are not, but the
comparison may be meaningless considering that both approches failed to
show good parameter recovery of ω₃.</p>
<p>Overall, these results suggest that we can use <code>hBayesDM</code>
for individual level analysis, which yields similar parameter estimates
from using TAPAS. At the same time, <code>hBayesDM</code> yields
posterior distributions instead of points estimates, which provide
additional information about the parameters.</p>
</div>
<div class="section level3">
<h3 id="recovery-with-hgf_ibrb">4.4. Recovery with <code>hgf_ibrb</code><a class="anchor" aria-label="anchor" href="#recovery-with-hgf_ibrb"></a>
</h3>
<p>For hierarchical Bayesian analysis, we applied <a href="../reference/hgf_ibrb.html"><code>hgf_ibrb</code></a> to the data
from 30 participants like below:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">output</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hgf_ibrb.html">hgf_ibrb</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">data</span>,</span>
<span>  niter <span class="op">=</span> <span class="fl">3000</span>,</span>
<span>  nwarmup <span class="op">=</span> <span class="fl">1500</span>,</span>
<span>  nchain <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  L <span class="op">=</span> <span class="fl">3</span>,</span>
<span>  kappa_lower <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1.0</span><span class="op">)</span>,</span>
<span>  kappa_upper <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1.0</span><span class="op">)</span>,</span>
<span>  omega_lower <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">9.0</span>, <span class="op">-</span><span class="fl">10.0</span><span class="op">)</span>, </span>
<span>  omega_upper <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1.0</span>, <span class="op">-</span><span class="fl">2.0</span><span class="op">)</span>,</span>
<span>  zeta_lower  <span class="op">=</span> <span class="fl">0.03</span>,</span>
<span>  zeta_upper  <span class="op">=</span> <span class="fl">4.0</span>,</span>
<span>  mu0   <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.0</span>, <span class="fl">1.0</span><span class="op">)</span>,</span>
<span>  inits <span class="op">=</span> <span class="st">"random"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="images/hgf_tutorial/hierarchical_recovery.png" alt="Figure 7. Parameter recovery with hBayesDM (hgf_ibrb, hierarchical Bayesian analysis)" width="90%"><p class="caption">
Figure 7. Parameter recovery with hBayesDM (hgf_ibrb, hierarchical
Bayesian analysis)
</p>
</div>
<p>Figure 7 shows that both ω₂ and ζ showed excellent parameter recovery
(ω₂: r = 0.99, p &lt; 1.27e-26; ζ: r = 0.95, p &lt; 2.62e-16), with
better correlation compared to <code>hgf_ibrb_single</code>.
Unfortunately, ω₃ still showed poor recovery (r = 0.06, p = 0.767), but
was slightly better than TAPAS considering that the results from TAPAS
had a negative correlation.</p>
<div class="figure" style="text-align: center">
<img src="images/hgf_tutorial/tapas_hierarchical_recovery.png" alt="Figure 8. TAPAS vs hgf_ibrb" width="90%"><p class="caption">
Figure 8. TAPAS vs hgf_ibrb
</p>
</div>
<p>In Figure 8, we compared TAPAS and hBayesDM (hierarchical Bayesian)
parameter estimates. Figure 8 shows that ω₂ and ζ parameter estimates of
hBayesDM and TAPAS are highly correlated with each other. Once again, ω₃
estimates of the two appraoches are not correlated with each other.</p>
<div class="figure" style="text-align: center">
<img src="images/hgf_tutorial/hierarchical_recovery_mu_posterior.png" alt="Figure 9. Parameter recovery" width="80%"><p class="caption">
Figure 9. Parameter recovery
</p>
</div>
<div class="figure" style="text-align: center">
<img src="images/hgf_tutorial/hierarchical_recovery_sigma_posterior.png" alt="Figure 9. Parameter recovery" width="80%"><p class="caption">
Figure 9. Parameter recovery
</p>
</div>
<p>Figure 9 shows the posterior distributions for group-level parameters
(group means and SDs) for ω₂, ω₃, and ζ, which is an advantage of using
MCMC for parameter estimation.</p>
<p>Overall, we conclude hBayesDM provides a user-friendly way of fitting
HGF models whose parameter estimates will be compatible with those from
TAPAS. In many cases, <a href="../reference/hgf_ibrb.html"><code>hgf_ibrb</code></a> should be
preferred over <a href="../reference/hgf_ibrb_single.html"><code>hgf_ibrb_single</code></a>
as hierarchical modeling framework further improves parameter recovery
compared to individual-level fits, presumably due to the benefits of
shrinkage <span class="citation">(Ahn et al., 2011; Kruschke,
2014)</span>.</p>
</div>
</div>
<div class="section level2">
<h2 id="appendix">5. Appendix<a class="anchor" aria-label="anchor" href="#appendix"></a>
</h2>
<div class="section level3">
<h3 id="specifying-initial-values">5.1. Specifying initial values<a class="anchor" aria-label="anchor" href="#specifying-initial-values"></a>
</h3>
<p><code>hBayesDM</code> provides various options for setting the
initial values (<code>inits</code>) of the parameters. The default
setting is <code>vb</code>, therefore <code>hBayesDM</code> will try to
fit the model with <a href="https://mc-stan.org/rstan/reference/stanmodel-method-vb.html" class="external-link">vb</a>
and then use the VB estimation as initial values for MCMC sampling. If
this VB estimation fails, <code>hBayesDM</code> defaults to using random
initial values instead.</p>
<p>Since the <code>vb</code> option has the issue of failing
stochastically, consider using the <code>random</code> option or
specifying the initial parameter values, like other models implemented
in hBayesDM.</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hgf_ibrb.html">hgf_ibrb</a></span><span class="op">(</span>data <span class="op">=</span> <span class="st">"example"</span>, inits <span class="op">=</span> <span class="st">"random"</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="poor-recovery-of-ω₃">5.2. Poor recovery of ω₃<a class="anchor" aria-label="anchor" href="#poor-recovery-of-%CF%89%E2%82%83"></a>
</h3>
<p>In <a href="#parameter-recovery">Section 4</a>, we showed that both
TAPAS and hBayesDM have failed to recover ω₃ properly. In fact, several
previous studies have also reported unreliable parameter estimation for
ω₃ <span class="citation">(Hein et al., 2021; Reed et al., 2020; Tecilla
et al., 2023)</span>. To our knowledge, there has been no systematic
analysis of the causes or the potential solutions. Therefore, we tried
various input designs and parameter settings to investigate under what
conditions we can achieve decent parameter recovery and ω₃ can produce
detectable behavioral differences.</p>
<p>The main issue with the data used in <a href="#parameter-recovery">Section 4</a> was that the individual
differences in ω₃ didn’t produce any meaningful differences in the
responses between the participants. Thus, we conducted parameter
recovery analyses multiple times using various input designs and
different ranges of ω₃ (-6.4 &lt; ω₃ &lt; -0.8) while fixing all other
parameters. Most promising results were found when we simply added more
volatility to the environment using the contingency schedule shown in
Figure 10.</p>
<div class="figure" style="text-align: center">
<img src="images/hgf_tutorial/contingency_schedule_om3.png" alt="Figure 10. Volatile Contingency Schedule" width="70%"><p class="caption">
Figure 10. Volatile Contingency Schedule
</p>
</div>
<p>The overall task design is similar to the Contingency Schedule
reported in <a href="#example-task">Section 1</a>. The total number of
trials is 400 and switches between stable and volatile phases. The main
difference is that number of trials for each stable and volatile phase
is 50 trials each, half the number of trials used in the previous
design. This leads to twice the number of shifts between stable and
volatile phases compared to the previous design.</p>
<div class="figure" style="text-align: center">
<img src="images/hgf_tutorial/om3_individual_recovery.png" alt="Figure 11. Parameter recovery" width="40%"><p class="caption">
Figure 11. Parameter recovery
</p>
</div>
<p>Figure 11 shows the result of parameter recovery of ω₃ using <a href="../reference/hgf_ibrb_single.html"><code>hgf_ibrb_single</code></a>.
Although the results are not perfect, we can see that the parameter
recovery results have improved compared to the results from the previous
design (Figure 5). This indicates that adding enough volatility to the
environment itself is a necessary step for capturing the individual
differences in parameters at high levels such as ω₃.</p>
</div>
<div class="section level3">
<h3 id="estimating-parameters-at-high-levels">5.3. Estimating parameters at high levels<a class="anchor" aria-label="anchor" href="#estimating-parameters-at-high-levels"></a>
</h3>
<p>Many studies have reported issues in estimating parameters at level 3
or above such as ω₃ <span class="citation">(Hein et al., 2021; Reed et
al., 2020; Tecilla et al., 2023)</span>. Below are some tips to mitigate
such issues.</p>
<ul>
<li><p><strong>Task design</strong>: As mentioned in the <a href="#poor-recovery-of-%CF%89%E2%82%83">previous section</a>, the researcher must
carefully design their experiment and check whether their task can
appropriately capture the individual differences in the parameters of
interest. Simulating experiment data and running parameter recovery
before actually conducting the experiment would be the best way to check
the task design.</p></li>
<li><p><strong>Parameter Boundaries</strong>: Setting appropriate
parameter boundaries is a crucial part of applying HGF. For instance, if
ω₃ is too low, then exp(ω₃) becomes very small (e.g., ω₃ = -8, exp(ω₃) ≈
0.000335), which means that level-3 updating has almost no influence on
the agent’s behavior.</p></li>
</ul>
</div>
</div>
<div class="section level2">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0" line-spacing="2">
<div id="ref-ahn2017hbayesdm" class="csl-entry">
Ahn, W.-Y., Haines, N., &amp; Zhang, L. (2017). Revealing
neurocomputational mechanisms of reinforcement learning and
decision-making with the hBayesDM package. <em>Computational
Psychiatry</em>, <em>1</em>, 24–57. http://doi.org/<a href="https://doi.org/10.1162/CPSY%5C_a%5C_00002" class="external-link">10.1162/CPSY\_a\_00002</a>
</div>
<div id="ref-ahn2011model" class="csl-entry">
Ahn, W.-Y., Krawitz, A., Kim, W., Busemeyer, J. R., &amp; Brown, J. W.
(2011). A model-based fMRI analysis with hierarchical bayesian parameter
estimation. <em>Journal of Neuroscience, Psychology, and Economics</em>,
<em>4</em>(2), 95.
</div>
<div id="ref-heathcote2015goodpractices" class="csl-entry">
Heathcote, A., Brown, S. D., &amp; Wagenmakers, E.-J. (2015). An
introduction to good practices in cognitive modeling. <em>Cognitive
Science</em>, <em>39</em>(5), 1069–1080.
</div>
<div id="ref-hein2021state" class="csl-entry">
Hein, T. P., Fockert, J. de, &amp; Herrojo Ruiz, M. (2021). State
anxiety biases estimates of uncertainty and impairs reward learning in
volatile environments. <em>NeuroImage</em>, <em>224</em>, 117424.
http://doi.org/<a href="https://doi.org/10.1016/j.neuroimage.2020.117424" class="external-link">10.1016/j.neuroimage.2020.117424</a>
</div>
<div id="ref-kruschke2014doing" class="csl-entry">
Kruschke, J. (2014). <em>Doing bayesian data analysis: A tutorial with
<span>R</span>, <span>JAGS</span>, and <span>S</span>tan</em>. Academic
Press.
</div>
<div id="ref-mathys2011bayesian" class="csl-entry">
Mathys, C., Daunizeau, J., Friston, K. J., &amp; Stephan, K. E. (2011).
A bayesian foundation for individual learning under uncertainty.
<em>Frontiers in Human Neuroscience</em>, <em>5</em>, 39.
</div>
<div id="ref-mathys2014hgf" class="csl-entry">
Mathys, C., Lomakina, E. I., Daunizeau, J., Iglesias, S., Brodersen, K.
H., Friston, K. J., &amp; Stephan, K. E. (2014). Uncertainty in
perception and the hierarchical gaussian filter. <em>Frontiers in Human
Neuroscience</em>, <em>8</em>, 825.
</div>
<div id="ref-reed2020paranoia" class="csl-entry">
Reed, E. J., Uddenberg, S., Suthaharan, P., Mathys, C. D., Taylor, J.
R., Groman, S. M., &amp; Corlett, P. R. (2020). Paranoia as a deficit in
non-social belief updating. <em>Elife</em>, <em>9</em>.
http://doi.org/<a href="https://doi.org/10.7554/eLife.56345" class="external-link">10.7554/eLife.56345</a>
</div>
<div id="ref-tecilla2023modulation" class="csl-entry">
Tecilla, M., Großbach, M., Gentile, G., Holland, P., Sporn, S.,
Antonini, A., &amp; Herrojo Ruiz, M. (2023). Modulation of motor vigor
by expectation of reward probability trial-by-trial is preserved in
healthy ageing and parkinson’s disease patients. <em>Journal of
Neuroscience</em>, <em>43</em>(10), 1757–1777. http://doi.org/<a href="https://doi.org/10.1523/JNEUROSCI.1583-22.2022" class="external-link">10.1523/JNEUROSCI.1583-22.2022</a>
</div>
<div id="ref-wilson2019tenrules" class="csl-entry">
Wilson, R. C., &amp; Collins, A. G. (2019). Ten simple rules for the
computational modeling of behavioral data. <em>eLife</em>, <em>8</em>,
e49547. http://doi.org/<a href="https://doi.org/10.7554/eLife.49547" class="external-link">10.7554/eLife.49547</a>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Woo-Young Ahn, Nate Haines, Lei Zhang.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

      </footer>
</div>






  </body>
</html>
